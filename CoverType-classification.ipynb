{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Import plotting packages:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hypothesis-testing methods:\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import chi2_contingency, f_oneway, kruskal\n",
    "from scikit_posthocs import posthoc_dunn, posthoc_tukey_hsd\n",
    "from pingouin import welch_anova, pairwise_gameshowell\n",
    "\n",
    "# Import data pre-processing packages:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler, LabelEncoder\n",
    "\n",
    "# Import supervised ML classification classes:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import performance-measuring methods:\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# documentation for multilabel_confusion_matrix:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix\n",
    "# dis very important:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' I noticed that I was getting several warnings when compiling TensorFlow regarding appropriate\n",
    "compiler flags; apparently, I'm not the only one, as evidenced by the following question from\n",
    "stackoverflow:\n",
    "https://stackoverflow.com/questions/66092421/how-to-rebuild-tensorflow-with-the-compiler-flags\n",
    "I have chosen to implement the commenter's solution.'''\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(11)\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(11)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cover_data = pd.read_csv('cover_data.csv')\n",
    "\n",
    "print(forest_cover_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cover_data = forest_cover_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cover_data[\"Cover_Type\"] = forest_cover_data['class'].map({ 1:'Spruce/Fir',  2:'Lodgepole Pine',  \\\n",
    "                                                                   3:'Ponderosa Pine',  4:'Cottonwood/Willow',\n",
    "                                                                   5:'Aspen', 6:'Douglas-fir', 7:'Krummholz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_cover_data.info())\n",
    "\n",
    "# Wow das a lotta columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_cover_data['class'].value_counts())\n",
    "print(forest_cover_data['Cover_Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_stats(column_name):\n",
    "    \n",
    "    # pluck off the data in column_name which belongs to each of the cover type classes, and store those\n",
    "    # data in their own Series:\n",
    "    temp_class1 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Spruce/Fir']\n",
    "    temp_class2 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Lodgepole Pine']\n",
    "    temp_class3 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Ponderosa Pine']\n",
    "    temp_class4 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Cottonwood/Willow']\n",
    "    temp_class5 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Aspen']\n",
    "    temp_class6 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Douglas-fir']\n",
    "    temp_class7 = forest_cover_data[column_name][forest_cover_data['Cover_Type'] == 'Krummholz']\n",
    "    \n",
    "    # get the data's quartiles:\n",
    "    quarts_class1 = np.fix(np.quantile(temp_class1, [0.25,0.5,0.75]))\n",
    "    quarts_class2 = np.fix(np.quantile(temp_class2, [0.25,0.5,0.75]))\n",
    "    quarts_class3 = np.fix(np.quantile(temp_class3, [0.25,0.5,0.75]))\n",
    "    quarts_class4 = np.fix(np.quantile(temp_class4, [0.25,0.5,0.75]))\n",
    "    quarts_class5 = np.fix(np.quantile(temp_class5, [0.25,0.5,0.75]))\n",
    "    quarts_class6 = np.fix(np.quantile(temp_class6, [0.25,0.5,0.75]))\n",
    "    quarts_class7 = np.fix(np.quantile(temp_class7, [0.25,0.5,0.75]))\n",
    "    \n",
    "    # gather up a dictionary for later conversion to a DataFrame:\n",
    "    tempdict = {'Cover_Type':['Spruce/Fir', 'Lodgepole Pine','Ponderosa Pine','Cottonwood/Willow','Aspen',\\\n",
    "                              'Douglas-fir','Krummholz'],\\\n",
    "                # calculate the average value of column_name for each class, and round to the nearest whole number:\n",
    "                'Avg':['{:.0f}'.format(temp_class1.mean()),'{:.0f}'.format(temp_class2.mean()),\\\n",
    "                       '{:.0f}'.format(temp_class3.mean()),'{:.0f}'.format(temp_class4.mean()),\\\n",
    "                       '{:.0f}'.format(temp_class5.mean()),'{:.0f}'.format(temp_class6.mean()),\\\n",
    "                       '{:.0f}'.format(temp_class7.mean())],\\\n",
    "                # calculate the median value of column_name for each class, and round to the nearest whole number:\n",
    "                'Median':['{:.0f}'.format(temp_class1.median()),'{:.0f}'.format(temp_class2.median()),\\\n",
    "                          '{:.0f}'.format(temp_class3.median()),'{:.0f}'.format(temp_class4.median()),\\\n",
    "                          '{:.0f}'.format(temp_class5.median()),'{:.0f}'.format(temp_class6.median()),\\\n",
    "                          '{:.0f}'.format(temp_class7.median())],\\\n",
    "                # find the minimum value of column_name for each class:\n",
    "                'Min':[min(temp_class1),min(temp_class2),min(temp_class3),min(temp_class4),\\\n",
    "                              min(temp_class5),min(temp_class6),min(temp_class7)],\\\n",
    "                # find the maximum value of column_name for each class:\n",
    "                'Max':[max(temp_class1),max(temp_class2),max(temp_class3),max(temp_class4),\\\n",
    "                              max(temp_class5),max(temp_class6),max(temp_class7)],\\\n",
    "                # gather up the quartiles calculated above:\n",
    "                'Quartiles':[quarts_class1,quarts_class2,quarts_class3,quarts_class4,\\\n",
    "                             quarts_class5,quarts_class6,quarts_class7],\\\n",
    "                # calculate the interquartile range (IQR) for each class:\n",
    "               'IQR':[quarts_class1[2]-quarts_class1[0],quarts_class2[2]-quarts_class2[0],\\\n",
    "                      quarts_class3[2]-quarts_class3[0],quarts_class4[2]-quarts_class4[0],\\\n",
    "                      quarts_class5[2]-quarts_class5[0],quarts_class6[2]-quarts_class6[0],\\\n",
    "                      quarts_class7[2]-quarts_class7[0]]}\n",
    "    \n",
    "    # convert from a dictionary to a DataFrame:\n",
    "    df_to_return = pd.DataFrame(tempdict)\n",
    "\n",
    "    # return the DataFrame:\n",
    "    return df_to_return\n",
    "\n",
    "# dis idea from\n",
    "# https://kiwidamien.github.io/stylish-pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dis code scrap storage:\n",
    "\n",
    "                ' 25th Per.':['{:.0f}'.format(np.percentile(temp_class1,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class2,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class3,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class4,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class5,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class6,0.25)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class7,0.25))],\\\n",
    "                ' 75th Per.':['{:.0f}'.format(np.percentile(temp_class1,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class2,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class3,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class4,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class5,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class6,0.75)),\\\n",
    "                             '{:.0f}'.format(np.percentile(temp_class7,0.75))]}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_distribution(column_name, nbins = 40, numofbins=40):\n",
    "    sns.set_context(\"notebook\", font_scale=0.8, rc={\"lines.linewidth\": 1.5})\n",
    "    \n",
    "    plt.figure(figsize=(9,5))\n",
    "    if column_name == 'Slope':\n",
    "        sns.histplot(forest_cover_data[column_name], bins = nbins, kde = True, discrete = True)\n",
    "    else:\n",
    "        sns.histplot(forest_cover_data[column_name], bins = nbins, kde = True)\n",
    "    if (column_name == 'Aspect') or (column_name == 'Slope'):\n",
    "        plt.xlabel(column_name + ' [degrees]', fontsize=13)\n",
    "    elif (column_name == 'Hillshade_9am') or (column_name == 'Hillshade_Noon') or (column_name == 'Hillshade_3pm'):\n",
    "        plt.xlabel(column_name, fontsize=13)\n",
    "    else:\n",
    "        plt.xlabel(column_name + ' [m]', fontsize=13)\n",
    "    plt.ylabel('Counts per Bin', fontsize=13)\n",
    "    plt.title(column_name + ' Distribution', fontsize=18)\n",
    "    \n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.violinplot(data = forest_cover_data, x = 'Cover_Type', y = column_name, \\\n",
    "                   order = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', \\\n",
    "                            'Aspen', 'Douglas-fir','Krummholz'])\n",
    "    plt.xlabel('Class', fontsize=13)\n",
    "    if (column_name == 'Aspect') or (column_name == 'Hillshade_9am') or \\\n",
    "    (column_name == 'Hillshade_noon') or (column_name == 'Hillshade_3pm'):\n",
    "        plt.ylabel(column_name, fontsize=13)\n",
    "    else:\n",
    "        plt.ylabel(column_name + ' [m]', fontsize=13)    \n",
    "    plt.title('Violin Plot of the ' + column_name + ' Distribution, Split by Class', fontsize=18)\n",
    "\n",
    "    temp_class1 = forest_cover_data[column_name][forest_cover_data['class'] == 1]\n",
    "    temp_class2 = forest_cover_data[column_name][forest_cover_data['class'] == 2]\n",
    "    temp_class3 = forest_cover_data[column_name][forest_cover_data['class'] == 3]\n",
    "    temp_class4 = forest_cover_data[column_name][forest_cover_data['class'] == 4]\n",
    "    temp_class5 = forest_cover_data[column_name][forest_cover_data['class'] == 5]\n",
    "    temp_class6 = forest_cover_data[column_name][forest_cover_data['class'] == 6]\n",
    "    temp_class7 = forest_cover_data[column_name][forest_cover_data['class'] == 7]\n",
    "   \n",
    "    plt.figure(figsize=(16,5))\n",
    "    if column_name == 'Slope':\n",
    "        sns.histplot(temp_class1, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'rosybrown', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class2, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'olivedrab', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class3, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'teal', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class4, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'silver', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class5, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'purple', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class6, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'maroon', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "        sns.histplot(temp_class7, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'darkblue', \\\n",
    "                     discrete = True, common_norm = False)\n",
    "    else:\n",
    "        sns.histplot(temp_class1, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'rosybrown', \\\n",
    "                 common_norm = False)\n",
    "        sns.histplot(temp_class2, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'olivedrab', \\\n",
    "                 common_norm = False)\n",
    "        sns.histplot(temp_class3, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'teal', \\\n",
    "                 common_norm = False)\n",
    "        if column_name == 'Horizontal_Distance_To_Hydrology':\n",
    "            sns.histplot(temp_class4, bins = 20, stat = 'density', kde = True, alpha = 0.50, color = 'silver', \\\n",
    "                         common_norm = False)\n",
    "        elif column_name == 'Vertical_Distance_To_Hydrology':\n",
    "            sns.histplot(temp_class4, bins = 20, stat = 'density', kde = True, alpha = 0.50, color = 'silver', \\\n",
    "                         common_norm = False)\n",
    "        else:\n",
    "            sns.histplot(temp_class4, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'silver', \\\n",
    "                         common_norm = False)\n",
    "        sns.histplot(temp_class5, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'purple', \\\n",
    "                     common_norm = False)\n",
    "        sns.histplot(temp_class6, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'maroon', \\\n",
    "                     common_norm = False)\n",
    "        sns.histplot(temp_class7, bins = numofbins, stat = 'density', kde = True, alpha = 0.50, color = 'darkblue', \\\n",
    "                     common_norm = False)\n",
    "    plt.legend(['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7'], fontsize=13)\n",
    "    if (column_name == 'Aspect') or (column_name == 'Hillshade_9am') or \\\n",
    "    (column_name == 'Hillshade_noon') or (column_name == 'Hillshade_3pm'):\n",
    "        plt.xlabel(column_name, fontsize=13)\n",
    "    else:\n",
    "        plt.xlabel(column_name + ' [m]', fontsize=13)\n",
    "    plt.ylabel('Density', fontsize=13)\n",
    "    plt.title(column_name + ' Distribution, Split by Class', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis idea from\n",
    "# https://kiwidamien.github.io/stylish-pandas.html\n",
    "\n",
    "def format_float(value):\n",
    "    return f'{value:,.2E}'\n",
    "\n",
    "pd.options.display.float_format = format_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_association_kwh(column_name):\n",
    "    \n",
    "    temp_class1 = forest_cover_data[column_name][forest_cover_data['class'] == 1]\n",
    "    temp_class2 = forest_cover_data[column_name][forest_cover_data['class'] == 2]\n",
    "    temp_class3 = forest_cover_data[column_name][forest_cover_data['class'] == 3]\n",
    "    temp_class4 = forest_cover_data[column_name][forest_cover_data['class'] == 4]\n",
    "    temp_class5 = forest_cover_data[column_name][forest_cover_data['class'] == 5]\n",
    "    temp_class6 = forest_cover_data[column_name][forest_cover_data['class'] == 6]\n",
    "    temp_class7 = forest_cover_data[column_name][forest_cover_data['class'] == 7]\n",
    "    \n",
    "    kh_tstat, kh_pval = kruskal(temp_class1, temp_class2, temp_class3, temp_class4, \\\n",
    "                                temp_class5, temp_class6, temp_class7)\n",
    "    print('Kruskal-Wallis H-test p-value for ' + column_name + '/class association: '+ str(kh_pval))\n",
    "    \n",
    "    dunntestresult = posthoc_dunn(a=forest_cover_data, val_col = column_name, group_col = 'Cover_Type', sort = True)\n",
    "    print('')\n",
    "    print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(dunntestresult, annot=True, center=0.001, fmt=\".2E\")\n",
    "\n",
    "# dis very helpful:\n",
    "#https://kiwidamien.github.io/stylish-pandas.html\n",
    "# also dis:\n",
    "#https://stackoverflow.com/questions/6913532/display-a-decimal-in-scientific-notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elevation, cannot use K-H test: different variances, different shapes of distributions, according to dis website:\n",
    "# http://www.biostathandbook.com/kruskalwallis.html\n",
    "# later on he recommends a Welch's ANOVA test:\n",
    "# http://www.biostathandbook.com/onewayanova.html#welch\n",
    "# From here:\n",
    "# https://stackoverflow.com/questions/50964427/welchs-anova-in-python\n",
    "# I was lead to \n",
    "# https://pingouin-stats.org/index.html\n",
    "# with documentation at:\n",
    "# https://pingouin-stats.org/generated/pingouin.welch_anova.html\n",
    "\n",
    "def test_for_association_welch(column_name):\n",
    "    \n",
    "    anal_of_var = welch_anova(data=forest_cover_data, dv=column_name, between='Cover_Type')\n",
    "    \n",
    "    pval_df = anal_of_var['p-unc']\n",
    "    pval = pval_df.iloc[0]\n",
    "    \n",
    "    print('Welch ANOVA p-value for ' + column_name + '/class association: '+ str(pval))\n",
    "    \n",
    "    gameshowelltestresult = pairwise_gameshowell(data=forest_cover_data, dv=column_name, between='Cover_Type').round(3)\n",
    "    ghtestresult = gameshowelltestresult[['A','B','mean(A)','mean(B)','T','pval']]\n",
    "    print('')\n",
    "    print(\"Games-Howell pairwise test for multiple comparisons of means:\")\n",
    "    print(ghtestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_stats = get_summary_stats('Elevation')\n",
    "print(elev_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Elevation', nbins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_pval, elev_dunntestresult = test_for_association('Elevation')\n",
    "print('Kruskal-Wallis H-test p-value for Elevation/class association: '+ str(elev_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(elev_dunntestresult)\n",
    "# dis very helpful:\n",
    "#https://stackoverflow.com/questions/4288973/whats-the-difference-between-s-and-d-in-python-string-formatting/56382046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis what aspect is:\n",
    "# https://en.wikipedia.org/wiki/Aspect_(geography)\n",
    "plot_a_distribution('Aspect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_pval, aspect_dunntestresult = test_for_association('Aspect')\n",
    "print('Kruskal-Wallis H-test p-value for Aspect/class association: '+ str(aspect_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(aspect_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Slope', nbins=70, numofbins=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_pval, slope_dunntestresult = test_for_association('Slope')\n",
    "print('Kruskal-Wallis H-test p-value for Slope/class association: '+ str(slope_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(slope_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Distance to Hydrology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Horizontal_Distance_To_Hydrology', nbins=30, numofbins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd2h_pval, hd2h_dunntestresult = test_for_association('Horizontal_Distance_To_Hydrology')\n",
    "print('Kruskal-Wallis H-test p-value for H.D.T.H./class association: '+ str(hd2h_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(hd2h_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical Distance to Hydrology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Vertical_Distance_To_Hydrology', numofbins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd2h_pval, vd2h_dunntestresult = test_for_association('Vertical_Distance_To_Hydrology')\n",
    "print('Kruskal-Wallis H-test p-value for V.D.T.H./class association: '+ str(vd2h_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(vd2h_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Distance to Roadways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Horizontal_Distance_To_Roadways')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd2r_pval, hd2r_dunntestresult = test_for_association('Horizontal_Distance_To_Roadways')\n",
    "print('Kruskal-Wallis H-test p-value for H.D.T.R./class association: '+ str(hd2r_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(hd2r_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Shade at 9AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Hillshade_9am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade0900_pval, shade0900_dunntestresult = test_for_association('Hillshade_9am')\n",
    "print('Kruskal-Wallis H-test p-value for Hill Shade at 9AM/class association: '+ str(shade0900_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(shade0900_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Shade at 12PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Hillshade_Noon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade1200_pval, shade1200_dunntestresult = test_for_association('Hillshade_Noon')\n",
    "print('Kruskal-Wallis H-test p-value for Hill Shade at 12PM/class association: '+ str(shade1200_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(shade1200_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Shade at 3PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Hillshade_3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade1500_pval, shade1500_dunntestresult = test_for_association('Hillshade_3pm')\n",
    "print('Kruskal-Wallis H-test p-value for Hill Shade at 3PM/class association: '+ str(shade1500_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(shade1500_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Distance to Fire Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_distribution('Horizontal_Distance_To_Fire_Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdtfp_pval, hdtfp_dunntestresult = test_for_association('Horizontal_Distance_To_Fire_Points')\n",
    "print('Kruskal-Wallis H-test p-value for H.D.T.F.P./class association: '+ str(hdtfp_pval))\n",
    "print(\"Dunn's pairwise test for multiple comparisons of mean rank sums:\")\n",
    "print(hdtfp_dunntestresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wilderness Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_cover_data.Wilderness_Area1.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_cover_data.Soil_Type1.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = forest_cover_data.iloc[:,0:53]\n",
    "labels = forest_cover_data.iloc[:,-1]\n",
    "\n",
    "labels_list = labels.to_list()\n",
    "\n",
    "ratio_train = 0.7\n",
    "ratio_valid = 0.15\n",
    "ratio_test = 0.15\n",
    "ratio_valid_adjusted = ratio_valid / (1 - ratio_test)\n",
    "\n",
    "vars_train, vars_test, labels_train, labels_test = \\\n",
    "    train_test_split(variables, labels, test_size = ratio_test, stratify = labels_list)\n",
    "\n",
    "print(labels.value_counts()/len(labels))\n",
    "print(labels_train.value_counts()/len(labels_train))\n",
    "print(labels_test.value_counts()/len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coltransform = ColumnTransformer([('minmax', MinMaxScaler(), \\\n",
    "                                 ['Elevation', 'Aspect', 'Slope', \\\n",
    "                                  'Horizontal_Distance_To_Hydrology', \\\n",
    "                                  'Vertical_Distance_To_Hydrology', \\\n",
    "                                  'Horizontal_Distance_To_Roadways', \\\n",
    "                                  'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \\\n",
    "                                  'Horizontal_Distance_To_Fire_Points'])], \\\n",
    "                                  remainder='passthrough')\n",
    "vars_scaled_train = coltransform.fit_transform(vars_train)\n",
    "vars_scaled_test = coltransform.transform(vars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "labelz_train=le.fit_transform(labels_train.astype(str))\n",
    "labelz_test=le.transform(labels_test.astype(str))\n",
    "\n",
    "labelz_train = to_categorical(labelz_train, dtype='int64')\n",
    "labelz_test = to_categorical(labelz_test, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars_scaled_train.shape, labelz_train.shape)\n",
    "print('')\n",
    "print(len(vars_scaled_train)*ratio_valid_adjusted)\n",
    "print('')\n",
    "print(vars_scaled_test.shape, labelz_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_a_model(xs):\n",
    "    \n",
    "    nvars = xs.shape[1]\n",
    "\n",
    "    thismodel = Sequential()\n",
    "    thismodel.add(InputLayer(input_shape=(nvars,)))\n",
    "    thismodel.add(Dense(512, activation='relu'))  # 512\n",
    "    thismodel.add(Dropout(0.01))\n",
    "    thismodel.add(Dense(256, activation='relu'))  #256\n",
    "    thismodel.add(Dropout(0.01))\n",
    "    thismodel.add(Dense(128, activation='relu'))  #128\n",
    "    thismodel.add(Dropout(0.01))\n",
    "    thismodel.add(Dense(64, activation='relu'))  #64\n",
    "    thismodel.add(Dropout(0.01))\n",
    "    thismodel.add(Dense(32, activation='relu')) #32\n",
    "    thismodel.add(Dropout(0.001))\n",
    "    thismodel.add(Dense(16, activation='relu')) #16\n",
    "    thismodel.add(Dropout(0.01))\n",
    "    thismodel.add(Dense(7, activation='softmax'))\n",
    "    thisoptimizer = Adam(learning_rate=0.005)  #0.005\n",
    "    thismodel.compile(loss='CategoricalCrossentropy',  metrics=['accuracy','Precision','Recall'], \\\n",
    "                      optimizer=thisoptimizer)\n",
    "    \n",
    "    return thismodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlistop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = build_a_model(vars_scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = nn_model.fit(vars_scaled_train, labelz_train, \\\n",
    "                             validation_split = ratio_valid_adjusted, \\\n",
    "                             shuffle = False, epochs = 310, batch_size = 800, \\\n",
    "                             callbacks=[earlistop], verbose = 1)\n",
    "model_predictions = nn_model.predict(vars_scaled_test)\n",
    "\n",
    "model_crossentropy, model_accuracy, model_precision, model_recall = \\\n",
    "     nn_model.evaluate(vars_scaled_test, labelz_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,16))\n",
    "plt.subplots_adjust(hspace=0.2,wspace=0.2)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(model_history.history['loss'], color = 'blue')\n",
    "plt.plot(model_history.history['val_loss'], color = 'crimson')\n",
    "plt.title('Model Categorical Cross-Entropy')\n",
    "plt.ylabel('Categorical Cross-Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Data', 'Validation Data'], loc='upper right')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(model_history.history['accuracy'], color = 'blue')\n",
    "plt.plot(model_history.history['val_accuracy'], color = 'crimson')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Data', 'Validation Data'], loc='lower right')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(model_history.history['precision'], color = 'blue')\n",
    "plt.plot(model_history.history['val_precision'], color = 'crimson')\n",
    "plt.title('Model Precision')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Data', 'Validation Data'], loc='lower right')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(model_history.history['recall'], color = 'blue')\n",
    "plt.plot(model_history.history['val_recall'], color = 'crimson')\n",
    "plt.title('Model Recall')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Data', 'Validation Data'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/save_and_serialize\n",
    "training_model.save('cover_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cover_classes = np.argmax(model_predictions, axis = 1)\n",
    "true_cover_classes = np.argmax(labelz_test, axis=1)\n",
    "print(classification_report(true_cover_classes, pred_cover_classes, zero_division = 'warn'))\n",
    "\n",
    "# in multiclass tasks, labels are binarized under a one-vs-rest way\n",
    "# In multilabel confusion matrix MCM ...\n",
    "# the count of true negatives is MCM[0,0]\n",
    "# false negatives is MCM [1,0]\n",
    "# true positives is MCM [1,1]\n",
    "# and false positives is MCM [0,1].\n",
    "# nomenclature: [row,column]\n",
    "#print(multilabel_confusion_matrix(true_cover_classes, pred_cover_classes))\n",
    "# The above was a decent idea but not apporpirate for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_mat = confusion_matrix(true_cover_classes, pred_cover_classes)\n",
    "print(confuse_mat)\n",
    "# from https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "\n",
    "cover_types = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', 'Aspen', 'Douglas-fir',\\\n",
    "               'Krummholz']\n",
    "\n",
    "confuse_df = pd.DataFrame(confuse_mat, index = cover_types, columns = cover_types)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(confuse_df, annot=True, center=1000, fmt=\"d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
